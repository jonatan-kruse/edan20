{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using Byte-Pair Encoding and a Unigram Language Model\n",
    "\n",
    "Author: Pierre Nugues with help from Marcus Klang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a tokenization program to handle subwords.\n",
    "\n",
    "In many scripts from Asia, like Chinese, Korean, or Japanese scripts, tokenization cannot rely on white spaces. The byte-pair encoding and the unigram language model are techniques that are now common in machine translation to carry out a tokenization at a subword level. Subword level tokenization shows better multilingual capabilities.\n",
    "\n",
    "You will follow two papers: \n",
    "* Subword Regularization: _Improving Neural Network Translation Models with Multiple Subword Candidates_ by Kudo (2018) (https://arxiv.org/pdf/1804.10959.pdf) and \n",
    "* _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020) (https://aclanthology.org/2020.findings-emnlp.414.pdf). \n",
    "\n",
    "In addition, you will start from a clear and easy-to-understand description in Google’s Neural Machine Translation System: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016). (Do not read them now)\n",
    "https://arxiv.org/abs/1609.08144\n",
    "\n",
    "You will use a small corpus make it easier to test and correct your code. Note also that you will use _characters_ and not _bytes_ in this lab as this is simpler to implement. For a complete program, see the link at the end.\n",
    "\n",
    "**In your report, be sure to answer all the questions. Please reuse the section titles of this notebook so that I can check your answers more easily**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an overall description of the subword tokenizers, read Sections 4 (introduction paragraph) and 4.1. in the paper on translation: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016), https://arxiv.org/abs/1609.08144.  \n",
    "\n",
    "In your report, in a few lines (10 to 15 lines or so) you will:\n",
    "\n",
    "1. Outline the difference with tokenization as you saw it during the course;\n",
    "2. Imagine how the tokens will be learned (this will developed in the rest of the lab);\n",
    "3. Summarize what could be the advantages for Asian languages, unknown words, and translation.\n",
    "\n",
    "Commenting Sections 4 and 4.1 in your report is **mandatory**. If you are curious, you can read the complete article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SentencePiece\n",
    "SentencePiece is the combination of BPE and a language model. To be sure you understand how it works, you will first run this code.\n",
    "These are the first cells from a larger program: https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "from Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/jonatan/anaconda/lib/python3.11/site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "--2023-09-16 14:25:10--  ftp://https/\n",
      "           => ‘.listing’\n",
      "Resolving https (https)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘https’\n",
      "//: Scheme missing.\n",
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2023-09-16 14:25:10--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt.4’\n",
      "\n",
      "botchan.txt.4       100%[===================>] 272.25K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-09-16 14:25:10 (7.39 MB/s) - ‘botchan.txt.4’ saved [278779/278779]\n",
      "\n",
      "FINISHED --2023-09-16 14:25:10--\n",
      "Total wall clock time: 0.1s\n",
      "Downloaded: 1 files, 272K in 0.04s (7.39 MB/s)\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece\n",
    "!wget https: // raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model from a corpus. Once trained, read the content of `m.vocab`. Be sure to undestand its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=botchan.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=9.18811 num_tokens=25552 num_tokens/piece=11.6145\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=9.18747 num_tokens=25552 num_tokens/piece=11.6145\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    '--input=botchan.txt --model_prefix=m --vocab_size=2000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386, 315, 27, 52, 1997, 227, 282, 14, 1058, 237, 22, 717, 1060, 443, 27, 67, 38, 20, 14]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('Tokenization using Byte-Pair Encoding'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁To'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.id_to_piece(386)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " ',',\n",
       " '.',\n",
       " '▁the',\n",
       " '▁I',\n",
       " 's',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁and',\n",
       " '▁of',\n",
       " '▁',\n",
       " 'ed',\n",
       " 'ing',\n",
       " '▁in',\n",
       " '▁was',\n",
       " '▁\"',\n",
       " '▁it',\n",
       " 't']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sp.id_to_piece(i) for i in range(20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁To', 'ke', 'n', 'i', 'z', 'ation', '▁us', 'ing', '▁By', 'te', '-', 'P', 'air', '▁E', 'n', 'c', 'o', 'd', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('Tokenization using Byte-Pair Encoding', out_type=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm to build the subwords from a corpus is a byte-pair encoding (BPE), due to Gage (1994). In the lab, you will first read two sections of more recent articles as they are easier to understand and specifically targeted to natural language processing.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.1 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 1 of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the byte-pair encoding (BPE) algorithm as described by Kudo (2018) and Bostrom and Durrett (2020) (Only BPE and not the unigram language model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now program a byte-pair encoding program in Python. You will do it step by step. The first part will be to extract the subwords from a corpus. Note that you will use the characters, not the bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tqdm as tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use a small corpus and then, if you have time, test your program on a larger one. Here we take the smallest novel from Selma Lagerlöf in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selma has been downloaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma/bannlyst.txt',\n",
       " 'Selma/gosta.txt',\n",
       " 'Selma/herrgard.txt',\n",
       " 'Selma/jerusalem.txt',\n",
       " 'Selma/kejsaren.txt',\n",
       " 'Selma/marbacka.txt',\n",
       " 'Selma/nils.txt',\n",
       " 'Selma/osynliga.txt',\n",
       " 'Selma/troll.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname)\n",
    "    for fname in\n",
    "    [\n",
    "        \"bannlyst.txt\",\n",
    "        \"gosta.txt\",\n",
    "        \"herrgard.txt\",\n",
    "        \"jerusalem.txt\",\n",
    "        \"kejsaren.txt\",\n",
    "        \"marbacka.txt\",\n",
    "        \"nils.txt\",\n",
    "        \"osynliga.txt\",\n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "\n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "\n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\n",
    "            \"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "\n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "\n",
    "SELMA_FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_PATH = '../../corpus/Selma.txt'\n",
    "FILE_PATH = 'Selma/herrgard.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the corpus and store it in the `corpus` string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the space sequences in `corpus`, including newlines and tabulations, and normalize them as one space. Use the `\\s` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus = re.sub(r'[\\s]+', ' ', corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selma Lagerlöf En herrgårdssägen Bokutgåva Albert Bonniers förlag, Stockholm 1899. I. Det var en skö'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code (one instruction) to split the corpus in a list of characters and store the results in `corpus_l`. This is just a type conversion. Given the input:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus = 'De senaste fem &aring;ren har cirka 25 000 unga'</span></pre>\n",
    "\n",
    "Return:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', 'e', ' ', 'f', 'e', 'm', ' ', ...]</span></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus_l = list(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'e', 'l', 'm', 'a', ' ', 'L', 'a', 'g', 'e', 'r', 'l', 'ö', 'f', ' ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_l[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the set of characters that will serve as initial subword tokens:\n",
    "\n",
    "1. Write a statement to extract the set of all the characters from `corpus_l`; \n",
    "2. Exclude the space from this set and call the resulting set: `char_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "char_set = set(corpus_l)\n",
    "char_set.remove(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ',', '-', '.', '1', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'X', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '»', 'Ä', 'Å', 'Ö', 'ä', 'å', 'é', 'ö', '–', '’']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from the previous question, write an `initial_vocabulary()` function taking the the `corpus_l` variable as input and returning the the set of all characters appearing in the corpus (the initial character set), deprived from the white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def initial_vocabulary(corpus_l):\n",
    "    vocabulary = set(corpus_l)\n",
    "    vocabulary.remove(' ')\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '1',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'X',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '»',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Ö',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'é',\n",
       " 'ö',\n",
       " '–',\n",
       " '’'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_vocabulary(corpus_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `pair_count()` function that takes a list of tokens as input, possibly single characters or subword tokens, and that counts the adjacent pairs (bigrams). You will implement these counts as dictionaries: The key will be a pair (tuple) of adjacent symbols and the value, its frequency. Remember that you cannot cross whitespaces, i.e. a pair cannot include a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input\n",
    "\n",
    "`['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "count_pairs should return a dictionary: \n",
    "\n",
    "\n",
    "`{('D', 'e'): 1, ('s', 'e'): 1, ('e', 'n'): 1, ('n', 'a'): 1, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def pair_count(corpus_l: list[str]):\n",
    "    corpus_l = list(filter(lambda x: x != ' ', corpus_l))\n",
    "    pairs = {}\n",
    "    for i in range(len(corpus_l) - 1):\n",
    "        pair = (corpus_l[i], corpus_l[i + 1])\n",
    "        if pair in pairs:\n",
    "            pairs[pair] += 1\n",
    "        else:\n",
    "            pairs[pair] = 1\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pair_count(corpus_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('S', 'e'): 7,\n",
       " ('e', 'l'): 592,\n",
       " ('l', 'm'): 45,\n",
       " ('m', 'a'): 501,\n",
       " ('a', 'L'): 1,\n",
       " ('L', 'a'): 8,\n",
       " ('a', 'g'): 572,\n",
       " ('g', 'e'): 711,\n",
       " ('e', 'r'): 1330,\n",
       " ('r', 'l'): 273,\n",
       " ('l', 'ö'): 81,\n",
       " ('ö', 'f'): 217,\n",
       " ('f', 'E'): 1,\n",
       " ('E', 'n'): 21,\n",
       " ('n', 'h'): 564,\n",
       " ('h', 'e'): 809,\n",
       " ('r', 'r'): 219,\n",
       " ('r', 'g'): 197,\n",
       " ('g', 'å'): 343,\n",
       " ('å', 'r'): 318,\n",
       " ('r', 'd'): 735,\n",
       " ('d', 's'): 205,\n",
       " ('s', 's'): 291,\n",
       " ('s', 'ä'): 213,\n",
       " ('ä', 'g'): 143,\n",
       " ('e', 'n'): 3520,\n",
       " ('n', 'B'): 1,\n",
       " ('B', 'o'): 2,\n",
       " ('o', 'k'): 56,\n",
       " ('k', 'u'): 573,\n",
       " ('u', 't'): 314,\n",
       " ('t', 'g'): 189,\n",
       " ('å', 'v'): 71,\n",
       " ('v', 'a'): 1426,\n",
       " ('a', 'A'): 3,\n",
       " ('A', 'l'): 24,\n",
       " ('l', 'b'): 60,\n",
       " ('b', 'e'): 314,\n",
       " ('r', 't'): 460,\n",
       " ('t', 'B'): 1,\n",
       " ('o', 'n'): 1610,\n",
       " ('n', 'n'): 1167,\n",
       " ('n', 'i'): 368,\n",
       " ('i', 'e'): 71,\n",
       " ('r', 's'): 516,\n",
       " ('s', 'f'): 95,\n",
       " ('f', 'ö'): 980,\n",
       " ('ö', 'r'): 1317,\n",
       " ('l', 'a'): 905,\n",
       " ('g', ','): 192,\n",
       " (',', 'S'): 1,\n",
       " ('S', 't'): 88,\n",
       " ('t', 'o'): 588,\n",
       " ('o', 'c'): 1223,\n",
       " ('c', 'k'): 794,\n",
       " ('k', 'h'): 60,\n",
       " ('h', 'o'): 1106,\n",
       " ('o', 'l'): 213,\n",
       " ('m', '1'): 1,\n",
       " ('1', '8'): 1,\n",
       " ('8', '9'): 1,\n",
       " ('9', '9'): 1,\n",
       " ('9', '.'): 1,\n",
       " ('.', 'I'): 126,\n",
       " ('I', '.'): 6,\n",
       " ('.', 'D'): 462,\n",
       " ('D', 'e'): 429,\n",
       " ('e', 't'): 2089,\n",
       " ('t', 'v'): 494,\n",
       " ('a', 'r'): 2009,\n",
       " ('r', 'e'): 724,\n",
       " ('n', 's'): 1139,\n",
       " ('s', 'k'): 854,\n",
       " ('k', 'ö'): 58,\n",
       " ('ö', 'n'): 102,\n",
       " ('h', 'ö'): 232,\n",
       " ('ö', 's'): 78,\n",
       " ('s', 't'): 1653,\n",
       " ('t', 'd'): 412,\n",
       " ('d', 'a'): 681,\n",
       " ('g', 'h'): 142,\n",
       " ('h', 'ä'): 201,\n",
       " ('ä', 'n'): 741,\n",
       " ('n', 'e'): 1042,\n",
       " ('e', 'm'): 317,\n",
       " ('m', 'o'): 277,\n",
       " ('o', 't'): 258,\n",
       " ('t', 's'): 630,\n",
       " ('s', 'l'): 175,\n",
       " ('l', 'u'): 81,\n",
       " ('t', 'e'): 1261,\n",
       " ('t', 'a'): 1394,\n",
       " ('a', 'f'): 612,\n",
       " ('f', 't'): 185,\n",
       " ('t', 'r'): 458,\n",
       " ('t', 't'): 2337,\n",
       " ('t', 'i'): 817,\n",
       " ('i', 'o'): 104,\n",
       " ('a', 'l'): 789,\n",
       " ('l', 'e'): 928,\n",
       " ('t', '.'): 305,\n",
       " ('.', 'P'): 22,\n",
       " ('P', 'å'): 14,\n",
       " ('å', 'd'): 318,\n",
       " ('d', 'e'): 4079,\n",
       " ('n', 't'): 558,\n",
       " ('i', 'd'): 555,\n",
       " ('n', 'f'): 321,\n",
       " ('f', 'a'): 228,\n",
       " ('a', 'n'): 2529,\n",
       " ('s', 'i'): 655,\n",
       " ('i', 'U'): 7,\n",
       " ('U', 'p'): 9,\n",
       " ('p', 's'): 41,\n",
       " ('s', 'a'): 677,\n",
       " ('a', 'e'): 89,\n",
       " ('t', 'h'): 664,\n",
       " ('ö', 'g'): 125,\n",
       " ('g', 't'): 350,\n",
       " ('t', ','): 409,\n",
       " (',', 'g'): 31,\n",
       " ('g', 'u'): 82,\n",
       " ('u', 'l'): 377,\n",
       " ('l', 't'): 229,\n",
       " ('v', 'å'): 94,\n",
       " ('å', 'n'): 362,\n",
       " ('i', 'n'): 1055,\n",
       " ('n', 'g'): 1329,\n",
       " ('g', 's'): 260,\n",
       " ('s', 'h'): 68,\n",
       " ('h', 'u'): 174,\n",
       " ('u', 's'): 190,\n",
       " ('s', ','): 45,\n",
       " (',', 's'): 601,\n",
       " ('s', 'o'): 672,\n",
       " ('o', 'm'): 1619,\n",
       " ('m', 's'): 159,\n",
       " ('o', 'd'): 244,\n",
       " ('d', 'u'): 182,\n",
       " ('u', 'n'): 676,\n",
       " ('n', 'd'): 1328,\n",
       " ('l', 'i'): 920,\n",
       " ('i', 'g'): 1168,\n",
       " ('a', 'm'): 494,\n",
       " ('m', 't'): 112,\n",
       " ('t', 'p'): 116,\n",
       " ('p', 'å'): 523,\n",
       " ('å', 'e'): 70,\n",
       " ('n', 'l'): 212,\n",
       " ('i', 't'): 537,\n",
       " ('n', 'ä'): 184,\n",
       " (',', 'l'): 34,\n",
       " ('l', 'å'): 190,\n",
       " ('t', 'b'): 206,\n",
       " ('b', 'o'): 161,\n",
       " ('o', 'r'): 743,\n",
       " ('a', 'i'): 108,\n",
       " ('n', 'u'): 225,\n",
       " ('t', 'k'): 147,\n",
       " ('k', 'a'): 843,\n",
       " ('f', 's'): 68,\n",
       " ('a', 'd'): 1617,\n",
       " ('n', '.'): 487,\n",
       " ('r', 'ä'): 398,\n",
       " ('ä', 't'): 180,\n",
       " ('r', 'u'): 369,\n",
       " ('k', 'i'): 119,\n",
       " ('c', 'h'): 1234,\n",
       " ('e', 'f'): 399,\n",
       " ('f', 'l'): 138,\n",
       " (',', 'm'): 188,\n",
       " ('m', 'e'): 829,\n",
       " ('t', 'f'): 366,\n",
       " ('n', 'a'): 1200,\n",
       " ('e', 's'): 782,\n",
       " ('f', 'd'): 90,\n",
       " ('n', 'm'): 192,\n",
       " ('a', 's'): 654,\n",
       " ('a', 'v'): 159,\n",
       " ('v', 'i'): 562,\n",
       " ('i', 'l'): 661,\n",
       " ('l', 'd'): 275,\n",
       " ('d', 'v'): 88,\n",
       " ('n', ','): 512,\n",
       " ('m', 'v'): 64,\n",
       " ('v', 'ä'): 357,\n",
       " ('ä', 'x'): 17,\n",
       " ('x', 't'): 9,\n",
       " ('e', 'd'): 887,\n",
       " ('d', 'ä'): 211,\n",
       " ('ä', 'r'): 819,\n",
       " ('r', ','): 244,\n",
       " (',', 'o'): 278,\n",
       " ('h', 's'): 207,\n",
       " ('m', 'p'): 39,\n",
       " ('å', 's'): 257,\n",
       " ('l', 's'): 249,\n",
       " ('n', 'k'): 476,\n",
       " ('k', 'r'): 193,\n",
       " ('ä', 'l'): 348,\n",
       " ('s', 'å'): 600,\n",
       " ('å', 'h'): 210,\n",
       " ('t', 'u'): 222,\n",
       " ('u', 'p'): 202,\n",
       " ('p', 'p'): 330,\n",
       " ('p', 'f'): 23,\n",
       " ('g', 'g'): 100,\n",
       " (',', 'a'): 556,\n",
       " ('a', 't'): 1762,\n",
       " ('l', 'l'): 1559,\n",
       " ('m', 'r'): 24,\n",
       " ('r', 'a'): 1336,\n",
       " ('i', 'ö'): 13,\n",
       " ('f', 'v'): 578,\n",
       " ('v', 'e'): 454,\n",
       " ('r', 'v'): 147,\n",
       " ('I', 'e'): 1,\n",
       " ('u', 'm'): 95,\n",
       " ('m', 'i'): 269,\n",
       " ('n', 'r'): 67,\n",
       " ('u', 'd'): 91,\n",
       " ('h', 'd'): 128,\n",
       " ('d', 'r'): 273,\n",
       " ('a', 'c'): 78,\n",
       " ('k', 's'): 88,\n",
       " ('t', 'm'): 203,\n",
       " ('g', 'o'): 437,\n",
       " ('f', 'f'): 62,\n",
       " ('f', 'e'): 42,\n",
       " ('e', '.'): 276,\n",
       " ('.', 'H'): 578,\n",
       " ('H', 'a'): 246,\n",
       " ('n', 'v'): 401,\n",
       " (',', 'v'): 56,\n",
       " ('k', 'e'): 490,\n",
       " ('r', 'k'): 302,\n",
       " ('f', 'i'): 182,\n",
       " ('s', 'e'): 359,\n",
       " ('e', 'e'): 258,\n",
       " ('H', 'å'): 1,\n",
       " ('b', 'a'): 231,\n",
       " ('r', 'h'): 371,\n",
       " ('h', 'a'): 1402,\n",
       " ('u', 'k'): 120,\n",
       " ('u', 'r'): 213,\n",
       " ('r', 'p'): 65,\n",
       " ('p', 'a'): 221,\n",
       " (',', 'd'): 238,\n",
       " ('g', 'v'): 71,\n",
       " ('u', 'g'): 73,\n",
       " ('e', 'b'): 154,\n",
       " ('t', 'ä'): 295,\n",
       " ('d', 'i'): 261,\n",
       " ('d', 'm'): 63,\n",
       " ('t', 'ö'): 69,\n",
       " ('k', 'l'): 136,\n",
       " ('l', 'ä'): 240,\n",
       " ('ä', 'd'): 173,\n",
       " ('d', 'd'): 249,\n",
       " ('n', 'b'): 289,\n",
       " ('e', 'k'): 204,\n",
       " ('k', 'v'): 73,\n",
       " ('ä', 'm'): 114,\n",
       " ('h', 'l'): 57,\n",
       " ('g', 'd'): 106,\n",
       " ('ä', 'k'): 37,\n",
       " ('k', 't'): 389,\n",
       " ('e', 'g'): 226,\n",
       " ('g', 'a'): 663,\n",
       " ('i', 's'): 396,\n",
       " ('m', ','): 133,\n",
       " ('r', 'f'): 253,\n",
       " ('o', 'f'): 85,\n",
       " ('a', ','): 364,\n",
       " ('o', 'p'): 71,\n",
       " ('r', 'i'): 701,\n",
       " ('i', 'f'): 238,\n",
       " ('f', 'b'): 16,\n",
       " ('d', 'o'): 149,\n",
       " ('h', 'p'): 32,\n",
       " ('p', 'r'): 97,\n",
       " ('a', 'b'): 142,\n",
       " ('h', 'y'): 31,\n",
       " ('y', 'l'): 15,\n",
       " ('l', 'o'): 271,\n",
       " ('ä', 's'): 198,\n",
       " ('b', 'ö'): 96,\n",
       " ('ö', 'c'): 2,\n",
       " ('r', '.'): 181,\n",
       " ('I', 'n'): 273,\n",
       " ('i', 'c'): 254,\n",
       " ('e', ','): 315,\n",
       " (',', 'k'): 47,\n",
       " ('k', 'o'): 521,\n",
       " ('l', 'h'): 95,\n",
       " ('n', 'o'): 640,\n",
       " ('m', '.'): 156,\n",
       " ('f', 'h'): 46,\n",
       " (',', 'e'): 34,\n",
       " ('r', 'b'): 125,\n",
       " ('b', 'r'): 167,\n",
       " ('d', 'k'): 57,\n",
       " ('l', ','): 62,\n",
       " ('k', ','): 55,\n",
       " (',', 'f'): 54,\n",
       " ('f', 'u'): 46,\n",
       " ('i', 'k'): 221,\n",
       " (',', 't'): 77,\n",
       " ('h', 'å'): 98,\n",
       " ('r', 'o'): 463,\n",
       " ('h', 'g'): 40,\n",
       " ('g', 'r'): 506,\n",
       " ('y', '.'): 1,\n",
       " ('.', '–'): 286,\n",
       " ('–', 'D'): 65,\n",
       " ('D', 'u'): 29,\n",
       " ('u', ','): 39,\n",
       " (',', 'H'): 3,\n",
       " ('H', 'e'): 142,\n",
       " ('e', '–'): 16,\n",
       " ('–', 's'): 150,\n",
       " ('e', 'h'): 676,\n",
       " ('n', '–'): 51,\n",
       " ('–', 'j'): 6,\n",
       " ('j', 'a'): 305,\n",
       " ('g', 'ä'): 61,\n",
       " ('m', 'm'): 460,\n",
       " ('l', 'v'): 28,\n",
       " ('g', '.'): 168,\n",
       " ('–', 'H'): 47,\n",
       " ('r', 'å'): 263,\n",
       " ('å', 'k'): 98,\n",
       " ('r', 'n'): 411,\n",
       " ('n', 'å'): 261,\n",
       " ('å', 'g'): 507,\n",
       " ('o', 'b'): 21,\n",
       " ('g', 'l'): 131,\n",
       " ('t', '?'): 23,\n",
       " ('?', '–'): 41,\n",
       " ('–', 'Å'): 13,\n",
       " ('Å', 'n'): 7,\n",
       " ('e', 'j'): 641,\n",
       " ('j', ','): 39,\n",
       " (',', 'i'): 44,\n",
       " ('g', '–'): 20,\n",
       " ('–', 'd'): 12,\n",
       " ('s', 'n'): 150,\n",
       " ('t', 'y'): 174,\n",
       " ('y', 's'): 74,\n",
       " ('g', 'n'): 81,\n",
       " ('d', '.'): 118,\n",
       " ('å', 'o'): 18,\n",
       " ('–', 'L'): 7,\n",
       " ('L', 'å'): 7,\n",
       " ('å', 't'): 325,\n",
       " ('d', 'å'): 234,\n",
       " ('å', '–'): 8,\n",
       " ('–', 'f'): 7,\n",
       " ('o', 'g'): 209,\n",
       " ('g', 'H'): 2,\n",
       " ('b', 'l'): 365,\n",
       " ('t', 'l'): 192,\n",
       " ('l', 'y'): 95,\n",
       " ('a', 'h'): 291,\n",
       " ('d', 'l'): 62,\n",
       " ('a', 'a'): 89,\n",
       " (',', 'j'): 39,\n",
       " ('g', 'i'): 214,\n",
       " (',', 'n'): 27,\n",
       " ('–', 'J'): 60,\n",
       " ('J', 'a'): 96,\n",
       " ('g', 'b'): 50,\n",
       " ('r', 'm'): 173,\n",
       " ('t', 'å'): 121,\n",
       " ('u', '.'): 15,\n",
       " ('.', 'J'): 54,\n",
       " ('g', 'k'): 71,\n",
       " ('j', 'l'): 52,\n",
       " ('i', 'a'): 51,\n",
       " ('s', 'j'): 118,\n",
       " ('j', 'ä'): 180,\n",
       " ('l', 'f'): 109,\n",
       " ('f', ':'): 1,\n",
       " (':', 'D'): 1,\n",
       " ('r', 'G'): 7,\n",
       " ('G', 'u'): 41,\n",
       " ('f', 'Å'): 1,\n",
       " ('Å', 'l'): 26,\n",
       " ('m', 'ä'): 125,\n",
       " ('y', 'c'): 215,\n",
       " ('–', 'F'): 8,\n",
       " ('F', 'ö'): 17,\n",
       " (',', 'Å'): 3,\n",
       " ('e', 'H'): 13,\n",
       " ('–', 't'): 6,\n",
       " ('o', 'e'): 11,\n",
       " ('t', 'j'): 92,\n",
       " ('.', 'M'): 199,\n",
       " ('M', 'i'): 8,\n",
       " ('r', 'j'): 169,\n",
       " ('j', 'u'): 279,\n",
       " ('u', 'e'): 24,\n",
       " ('i', 'h'): 97,\n",
       " ('a', '–'): 18,\n",
       " ('e', 'Å'): 7,\n",
       " ('h', 't'): 52,\n",
       " ('r', 'ö'): 207,\n",
       " ('g', 'f'): 157,\n",
       " ('f', 'r'): 423,\n",
       " ('m', 'f'): 108,\n",
       " ('r', 'H'): 27,\n",
       " ('e', 'o'): 132,\n",
       " ('h', 'v'): 258,\n",
       " ('j', 'e'): 75,\n",
       " ('e', 'ö'): 50,\n",
       " ('m', 'd'): 182,\n",
       " ('a', 'k'): 227,\n",
       " ('h', 'j'): 72,\n",
       " ('l', 'p'): 59,\n",
       " ('m', 'u'): 60,\n",
       " ('–', 'S'): 7,\n",
       " ('l', 'k'): 119,\n",
       " ('l', 'n'): 57,\n",
       " ('h', 'm'): 42,\n",
       " ('d', 'h'): 152,\n",
       " ('p', 't'): 61,\n",
       " ('å', 'a'): 100,\n",
       " ('f', 'å'): 135,\n",
       " ('a', '.'): 265,\n",
       " ('g', 'p'): 57,\n",
       " ('r', 'y'): 82,\n",
       " ('–', 'T'): 3,\n",
       " ('T', 'a'): 8,\n",
       " ('k', 'y'): 103,\n",
       " ('y', 'm'): 28,\n",
       " ('–', 'a'): 5,\n",
       " ('a', 'o'): 137,\n",
       " ('u', 'i'): 13,\n",
       " ('g', 'ö'): 78,\n",
       " ('k', 'n'): 82,\n",
       " ('a', 'p'): 169,\n",
       " ('ö', 'p'): 37,\n",
       " ('p', 'n'): 32,\n",
       " ('k', 'p'): 20,\n",
       " ('f', 'y'): 21,\n",
       " ('y', 'r'): 93,\n",
       " ('j', 'g'): 30,\n",
       " ('s', 'p'): 169,\n",
       " ('p', 'e'): 171,\n",
       " ('n', 'j'): 29,\n",
       " ('j', 'h'): 39,\n",
       " ('å', 'l'): 156,\n",
       " ('y', 'd'): 61,\n",
       " ('u', 'v'): 49,\n",
       " ('e', 'a'): 135,\n",
       " ('i', 'v'): 33,\n",
       " ('i', 'F'): 2,\n",
       " ('F', 'a'): 5,\n",
       " ('v', 'u'): 43,\n",
       " ('l', 'r'): 26,\n",
       " ('.', 'Å'): 9,\n",
       " ('f', 'o'): 132,\n",
       " ('m', 'h'): 255,\n",
       " ('m', 'M'): 1,\n",
       " ('M', 'u'): 20,\n",
       " ('y', 't'): 72,\n",
       " (',', 'b'): 61,\n",
       " ('l', '.'): 56,\n",
       " ('.', 'T'): 22,\n",
       " ('e', 'x'): 6,\n",
       " ('x', 'a'): 11,\n",
       " (',', 'ä'): 27,\n",
       " ('t', 'n'): 131,\n",
       " ('j', 'b'): 25,\n",
       " ('å', 'M'): 6,\n",
       " ('f', '.'): 32,\n",
       " ('h', 'Å'): 1,\n",
       " ('m', 'g'): 76,\n",
       " ('i', 'Å'): 1,\n",
       " ('s', 'ö'): 73,\n",
       " ('s', 'r'): 60,\n",
       " ('h', 'h'): 158,\n",
       " ('s', 'm'): 108,\n",
       " ('–', 'M'): 19,\n",
       " ('M', 'e'): 181,\n",
       " ('å', ','): 85,\n",
       " ('t', 'M'): 5,\n",
       " ('å', 'j'): 13,\n",
       " ('u', 'f'): 71,\n",
       " ('–', 'å'): 2,\n",
       " ('d', 'ö'): 95,\n",
       " ('ö', 'd'): 95,\n",
       " ('d', ','): 168,\n",
       " ('.', 'B'): 20,\n",
       " ('B', 'e'): 15,\n",
       " ('j', 'v'): 73,\n",
       " ('u', '–'): 6,\n",
       " ('t', '–'): 20,\n",
       " ('j', 'm'): 39,\n",
       " ('s', '?'): 5,\n",
       " ('j', '–'): 5,\n",
       " (',', 'h'): 238,\n",
       " ('d', 'M'): 2,\n",
       " ('T', 'ä'): 3,\n",
       " ('f', ','): 20,\n",
       " ('å', 'i'): 33,\n",
       " ('j', 'o'): 93,\n",
       " ('d', 'b'): 63,\n",
       " ('i', 'V'): 5,\n",
       " ('V', 'ä'): 3,\n",
       " ('m', 'l'): 104,\n",
       " ('u', 'b'): 49,\n",
       " ('b', 'b'): 15,\n",
       " ('d', 't'): 197,\n",
       " ('o', ','): 23,\n",
       " ('j', 'f'): 39,\n",
       " ('m', 'n'): 99,\n",
       " ('H', 'u'): 19,\n",
       " ('m', 'å'): 171,\n",
       " ('H', 'o'): 279,\n",
       " ('m', 'k'): 68,\n",
       " ('g', 'm'): 72,\n",
       " (',', 'u'): 47,\n",
       " ('s', 'd'): 41,\n",
       " ('j', 's'): 62,\n",
       " ('l', 'j'): 102,\n",
       " ('u', 'ä'): 9,\n",
       " ('f', 'ä'): 75,\n",
       " ('n', 'y'): 62,\n",
       " ('m', '!'): 8,\n",
       " ('!', 'H'): 20,\n",
       " ('p', 'o'): 20,\n",
       " ('.', 'S'): 60,\n",
       " ('S', 'å'): 41,\n",
       " ('r', 'Å'): 2,\n",
       " ('u', 'h'): 25,\n",
       " ('b', 'i'): 39,\n",
       " ('.', 'V'): 14,\n",
       " ('V', 'i'): 27,\n",
       " ('i', 'ä'): 4,\n",
       " ('o', 'j'): 6,\n",
       " ('i', 'r'): 38,\n",
       " ('k', 'd'): 21,\n",
       " ('a', 'ä'): 19,\n",
       " ('i', 'p'): 38,\n",
       " ('ä', 'c'): 87,\n",
       " ('å', 'm'): 80,\n",
       " ('h', 'i'): 60,\n",
       " ('n', '!'): 14,\n",
       " ('f', 'n'): 26,\n",
       " ('å', 'p'): 15,\n",
       " ('h', 'f'): 77,\n",
       " ('.', 'O'): 144,\n",
       " ('O', 'c'): 138,\n",
       " ('å', 'f'): 72,\n",
       " ('e', 'i'): 137,\n",
       " ('e', 'p'): 140,\n",
       " ('i', 'b'): 15,\n",
       " ('o', 'h'): 19,\n",
       " ('k', '.'): 51,\n",
       " ('e', 'v'): 204,\n",
       " ('D', 'ä'): 24,\n",
       " ('h', 'r'): 28,\n",
       " ('k', '!'): 1,\n",
       " ('!', 'Å'): 2,\n",
       " ('n', 'ö'): 76,\n",
       " ('ö', 'j'): 67,\n",
       " ('u', 'a'): 4,\n",
       " ('d', 'g'): 58,\n",
       " ('n', 'p'): 96,\n",
       " (',', 'å'): 4,\n",
       " ('e', 'u'): 56,\n",
       " ('–', 'V'): 17,\n",
       " ('V', 'a'): 8,\n",
       " ('k', 'm'): 7,\n",
       " ('o', 'ä'): 10,\n",
       " ('n', '?'): 21,\n",
       " ('?', 'J'): 3,\n",
       " ('s', 'v'): 139,\n",
       " ('b', 'ä'): 68,\n",
       " ('s', 'b'): 49,\n",
       " ('a', 'u'): 70,\n",
       " ('j', 'ö'): 34,\n",
       " ('p', '.'): 10,\n",
       " ('r', '–'): 14,\n",
       " ('.', '»'): 2,\n",
       " ('»', 'Å'): 1,\n",
       " ('j', 'd'): 53,\n",
       " ('g', 'j'): 53,\n",
       " ('.', 'L'): 12,\n",
       " ('!', '–'): 9,\n",
       " ('–', 'h'): 6,\n",
       " ('j', 'k'): 43,\n",
       " ('n', 'Å'): 1,\n",
       " ('h', 'b'): 69,\n",
       " ('H', 'v'): 38,\n",
       " ('l', '?'): 2,\n",
       " ('å', 'b'): 43,\n",
       " ('s', 'u'): 34,\n",
       " ('B', 'a'): 9,\n",
       " ('n', 'H'): 7,\n",
       " ('o', 's'): 141,\n",
       " ('–', 'e'): 1,\n",
       " ('n', 'M'): 1,\n",
       " ('.', 'N'): 39,\n",
       " ('N', 'å'): 7,\n",
       " ('d', 'j'): 74,\n",
       " ('–', 'G'): 5,\n",
       " ('G', 'j'): 1,\n",
       " ('Å', 'j'): 2,\n",
       " ('v', 'o'): 90,\n",
       " ('m', 'ö'): 80,\n",
       " ('ö', 't'): 56,\n",
       " ('a', 'å'): 22,\n",
       " ('y', 'n'): 62,\n",
       " ('.', '.'): 12,\n",
       " ('s', 'y'): 60,\n",
       " ('n', 'G'): 9,\n",
       " ('t', 'Å'): 3,\n",
       " ('ö', '.'): 6,\n",
       " ('k', 'ä'): 152,\n",
       " ('D', 'å'): 60,\n",
       " ('å', 'Å'): 2,\n",
       " ('i', 'm'): 52,\n",
       " ('d', 'f'): 102,\n",
       " ('ö', 'l'): 71,\n",
       " ('b', 'u'): 30,\n",
       " ('d', '–'): 15,\n",
       " ('.', 'A'): 24,\n",
       " ('A', 't'): 1,\n",
       " ('–', 'E'): 3,\n",
       " ('m', 'j'): 21,\n",
       " ('j', 't'): 27,\n",
       " (',', 'p'): 9,\n",
       " ('v', 'r'): 12,\n",
       " ('d', 'p'): 24,\n",
       " ('p', ','): 18,\n",
       " ('J', 'u'): 19,\n",
       " ('å', 'H'): 7,\n",
       " ('p', 'l'): 59,\n",
       " ('h', 'k'): 51,\n",
       " ('ä', 'p'): 33,\n",
       " ('a', 'ö'): 42,\n",
       " ('k', 'k'): 7,\n",
       " ('l', 'g'): 27,\n",
       " ('å', 'ö'): 11,\n",
       " ('.', 'G'): 20,\n",
       " ('å', '.'): 27,\n",
       " ('B', 'l'): 52,\n",
       " ('t', 'H'): 4,\n",
       " ('ä', 'f'): 53,\n",
       " ('.', 'F'): 42,\n",
       " ('F', 'l'): 10,\n",
       " ('h', 'n'): 44,\n",
       " ('l', 'H'): 3,\n",
       " ('–', 'N'): 25,\n",
       " ('N', 'u'): 23,\n",
       " ('r', 'F'): 2,\n",
       " ('F', 'r'): 17,\n",
       " ('s', 'H'): 3,\n",
       " ('F', 'i'): 5,\n",
       " ('l', '–'): 5,\n",
       " ('–', 'm'): 5,\n",
       " ('k', '–'): 3,\n",
       " ('d', '?'): 4,\n",
       " ('?', 'H'): 28,\n",
       " ('k', 'f'): 25,\n",
       " ('o', 'v'): 9,\n",
       " ('s', 'c'): 11,\n",
       " ('k', 'j'): 8,\n",
       " ('.', 'U'): 16,\n",
       " ('U', 'n'): 12,\n",
       " ('h', 'H'): 2,\n",
       " ('u', 'u'): 2,\n",
       " ('p', 'u'): 15,\n",
       " ('j', 'H'): 1,\n",
       " ('m', 'y'): 74,\n",
       " ('y', 'g'): 49,\n",
       " ('j', 'p'): 7,\n",
       " ('o', 'o'): 7,\n",
       " ('p', 'h'): 19,\n",
       " ('A', 'k'): 1,\n",
       " ('f', 'F'): 1,\n",
       " ('ö', 'm'): 79,\n",
       " ('m', 'b'): 41,\n",
       " ('o', 'i'): 11,\n",
       " (',', 'r'): 17,\n",
       " ('e', '!'): 6,\n",
       " ('!', 'D'): 13,\n",
       " ('e', 'å'): 12,\n",
       " ('k', 'b'): 13,\n",
       " ('I', 'h'): 1,\n",
       " ('i', 'u'): 5,\n",
       " ('y', 'k'): 4,\n",
       " ('.', 'K'): 21,\n",
       " ('K', 'o'): 5,\n",
       " ('K', 'a'): 18,\n",
       " ('r', 'B'): 21,\n",
       " ('t', 'c'): 2,\n",
       " ('c', 'i'): 6,\n",
       " ('u', 'B'): 28,\n",
       " ('s', 'V'): 2,\n",
       " ('S', 'j'): 1,\n",
       " ('f', 'g'): 11,\n",
       " ('s', 'g'): 21,\n",
       " ('y', 'i'): 3,\n",
       " ('å', 'u'): 21,\n",
       " ('d', 'H'): 1,\n",
       " ('g', 'c'): 1,\n",
       " ('s', '.'): 37,\n",
       " ('d', 'n'): 35,\n",
       " ('O', 'm'): 26,\n",
       " ('o', 'a'): 15,\n",
       " ('M', 'a'): 19,\n",
       " ('f', 'p'): 3,\n",
       " ('.', 'C'): 1,\n",
       " ('C', 'i'): 1,\n",
       " ('r', '!'): 6,\n",
       " ('I', 'b'): 3,\n",
       " ('e', 'c'): 11,\n",
       " ('K', 'u'): 4,\n",
       " ('p', 'm'): 14,\n",
       " ('?', 'T'): 2,\n",
       " ('T', 'r'): 4,\n",
       " ('?', 'K'): 2,\n",
       " ('J', 'o'): 3,\n",
       " ('V', 'å'): 4,\n",
       " ('–', 'o'): 2,\n",
       " ('L', 'ä'): 4,\n",
       " ('y', ','): 1,\n",
       " ('p', 'i'): 44,\n",
       " ('t', '!'): 10,\n",
       " ('–', 'B'): 4,\n",
       " ('e', 'z'): 1,\n",
       " ('z', '?'): 1,\n",
       " ('p', 'b'): 4,\n",
       " ('e', 'I'): 23,\n",
       " ('r', '?'): 11,\n",
       " ('–', 'Ä'): 4,\n",
       " ('Ä', 'r'): 7,\n",
       " ('?', 'V'): 6,\n",
       " ('i', ','): 5,\n",
       " ('f', 'k'): 21,\n",
       " ('e', '?'): 11,\n",
       " ('e', 'ä'): 37,\n",
       " ('N', 'e'): 22,\n",
       " ('n', 'I'): 17,\n",
       " ('T', 'å'): 2,\n",
       " ('o', 'H'): 1,\n",
       " ('ö', 'k'): 65,\n",
       " ('k', ';'): 1,\n",
       " (';', 'e'): 1,\n",
       " ('N', 'ä'): 5,\n",
       " ('.', '_'): 15,\n",
       " ('_', '_'): 119,\n",
       " ('_', 'I'): 5,\n",
       " ('I', 'I'): 6,\n",
       " ('.', 'E'): 20,\n",
       " (',', 'ö'): 9,\n",
       " ('ö', 'a'): 3,\n",
       " ('M', 'ä'): 3,\n",
       " ('a', 'y'): 2,\n",
       " ('y', 'p'): 4,\n",
       " ('m', 'H'): 1,\n",
       " ('u', 'x'): 9,\n",
       " ('x', 'i'): 3,\n",
       " ('p', 'v'): 3,\n",
       " ('a', 'M'): 2,\n",
       " ('å', 'ä'): 5,\n",
       " ('b', 'y'): 10,\n",
       " ('.', 'Ö'): 6,\n",
       " ('Ö', 'f'): 1,\n",
       " ('p', 'g'): 4,\n",
       " ('ö', 'b'): 6,\n",
       " ('G', 'a'): 1,\n",
       " ('.', 'Ä'): 7,\n",
       " ('Ä', 'f'): 1,\n",
       " ('h', 'M'): 1,\n",
       " ('t', 'G'): 6,\n",
       " ('e', 'G'): 5,\n",
       " ('x', 'n'): 2,\n",
       " ('n', 'U'): 1,\n",
       " ('j', 'r'): 18,\n",
       " ('V', 'e'): 5,\n",
       " ('o', 'å'): 3,\n",
       " ('G', 'e'): 15,\n",
       " ('b', 'å'): 19,\n",
       " ('ö', ','): 7,\n",
       " ('y', 'f'): 29,\n",
       " ('a', 'x'): 8,\n",
       " ('x', 'l'): 3,\n",
       " ('N', 'i'): 7,\n",
       " ('H', 'ö'): 10,\n",
       " ('x', 'e'): 9,\n",
       " ('S', 'n'): 2,\n",
       " ('o', 'y'): 1,\n",
       " ('j', '.'): 10,\n",
       " ('å', 'å'): 6,\n",
       " ('.', 'R'): 5,\n",
       " ('R', 'å'): 10,\n",
       " ('a', 'V'): 2,\n",
       " ('a', 'D'): 1,\n",
       " ('D', 'a'): 6,\n",
       " (',', 'I'): 9,\n",
       " ('O', 'v'): 1,\n",
       " ('b', 'j'): 11,\n",
       " ('S', 'y'): 2,\n",
       " ('i', 'R'): 7,\n",
       " ('d', 'V'): 1,\n",
       " ('m', 'V'): 1,\n",
       " ('L', 'i'): 16,\n",
       " ('G', 'r'): 5,\n",
       " ('j', 'i'): 12,\n",
       " ('o', 'ö'): 2,\n",
       " ('m', '–'): 11,\n",
       " ('G', 'o'): 2,\n",
       " ('a', 'G'): 3,\n",
       " ('l', '!'): 2,\n",
       " ('j', 'n'): 8,\n",
       " ('–', 'I'): 22,\n",
       " ('–', 'k'): 4,\n",
       " ('g', '?'): 8,\n",
       " ('?', 'D'): 7,\n",
       " ('a', '?'): 12,\n",
       " ('–', 'A'): 5,\n",
       " ('–', 'O'): 15,\n",
       " ('m', '?'): 7,\n",
       " ('u', 'j'): 1,\n",
       " ('h', 'I'): 9,\n",
       " ('o', 'u'): 13,\n",
       " ('–', '–'): 10,\n",
       " ('–', '_'): 2,\n",
       " ('I', 'V'): 1,\n",
       " ('V', '.'): 2,\n",
       " ('v', 'ö'): 1,\n",
       " ('S', 'o'): 22,\n",
       " ('T', 'ö'): 3,\n",
       " ('T', 'i'): 2,\n",
       " ('p', 'ä'): 14,\n",
       " ('I', 'd'): 9,\n",
       " ('K', 'r'): 2,\n",
       " ('R', 'i'): 1,\n",
       " ('A', 'f'): 2,\n",
       " ('i', 'G'): 1,\n",
       " ('Ö', 'g'): 6,\n",
       " ('Ä', 'n'): 4,\n",
       " ('T', 'v'): 2,\n",
       " ('u', 'c'): 7,\n",
       " ('E', 'g'): 1,\n",
       " ('h', ','): 2,\n",
       " ('–', 'U'): 1,\n",
       " ('U', 's'): 1,\n",
       " ('d', 'G'): 2,\n",
       " ('f', 'm'): 8,\n",
       " ('a', '!'): 2,\n",
       " ('!', 'I'): 3,\n",
       " ('H', 'ä'): 15,\n",
       " ('y', 'ö'): 1,\n",
       " ('ä', 'b'): 3,\n",
       " ('E', 'j'): 1,\n",
       " ('E', 'f'): 2,\n",
       " ('m', 'G'): 1,\n",
       " ('T', 'o'): 2,\n",
       " ('N', 'y'): 1,\n",
       " ('P', 'r'): 5,\n",
       " ('y', 'h'): 9,\n",
       " ('L', 'j'): 3,\n",
       " ('a', 'j'): 13,\n",
       " ('S', 'l'): 2,\n",
       " ('y', 'x'): 1,\n",
       " ('x', 'h'): 1,\n",
       " ('g', 'I'): 6,\n",
       " ('p', 'ö'): 6,\n",
       " ('R', 'u'): 2,\n",
       " ('ä', 'h'): 3,\n",
       " ('a', 'C'): 1,\n",
       " ('C', 'a'): 2,\n",
       " ('f', 'L'): 1,\n",
       " ('t', 'L'): 1,\n",
       " ('S', 'a'): 2,\n",
       " ('d', 'I'): 7,\n",
       " ('o', '.'): 8,\n",
       " ('t', 'I'): 20,\n",
       " ('a', 'I'): 6,\n",
       " ('A', 'c'): 3,\n",
       " ('d', '!'): 2,\n",
       " ('a', ';'): 1,\n",
       " (';', 'h'): 2,\n",
       " ('Å', 'k'): 3,\n",
       " ('x', 'f'): 1,\n",
       " ('?', 'I'): 4,\n",
       " ('p', 'd'): 5,\n",
       " ('g', '!'): 4,\n",
       " ('–', 'R'): 1,\n",
       " ('R', 'ö'): 1,\n",
       " ('!', 'T'): 1,\n",
       " ('Å', 'g'): 1,\n",
       " ('!', 'J'): 1,\n",
       " ('?', 'B'): 2,\n",
       " ('B', 'r'): 1,\n",
       " ('y', 'a'): 4,\n",
       " ('l', 'I'): 7,\n",
       " ('i', '–'): 1,\n",
       " ('ä', ','): 1,\n",
       " ('_', 'D'): 4,\n",
       " ('f', 'j'): 3,\n",
       " ('d', 'y'): 6,\n",
       " ('Å', 'v'): 1,\n",
       " ('M', 'o'): 11,\n",
       " ('h', 'L'): 1,\n",
       " ('s', 'C'): 1,\n",
       " ('I', 'm'): 1,\n",
       " ('y', 'o'): 2,\n",
       " ('m', 'I'): 5,\n",
       " ('p', 'y'): 1,\n",
       " ('I', 'k'): 1,\n",
       " ('u', 'L'): 1,\n",
       " ('k', 'å'): 7,\n",
       " ('e', 'L'): 3,\n",
       " ('Å', 's'): 1,\n",
       " ('i', 'j'): 7,\n",
       " ('r', 'L'): 1,\n",
       " ('–', 'b'): 2,\n",
       " ('B', 'å'): 1,\n",
       " ('?', 'M'): 5,\n",
       " ('F', 'o'): 2,\n",
       " ('u', '!'): 2,\n",
       " ('!', 'B'): 3,\n",
       " ('B', 'ä'): 2,\n",
       " ('l', 'é'): 7,\n",
       " ('é', 'n'): 6,\n",
       " ('K', 'l'): 2,\n",
       " ('_', 'V'): 4,\n",
       " ('r', 'A'): 22,\n",
       " ('A', 'n'): 30,\n",
       " ('a', 'S'): 27,\n",
       " ('k', 'g'): 7,\n",
       " ('G', 'å'): 3,\n",
       " ('u', 'å'): 2,\n",
       " ('x', 'å'): 1,\n",
       " ('h', 'c'): 1,\n",
       " ('å', 'I'): 15,\n",
       " ('u', '?'): 7,\n",
       " ('e', ';'): 1,\n",
       " ('F', 'e'): 1,\n",
       " (',', '–'): 5,\n",
       " ('p', 'k'): 4,\n",
       " ('?', 'E'): 3,\n",
       " ('E', 't'): 2,\n",
       " ('V', 'I'): 3,\n",
       " ('n', 'A'): 3,\n",
       " ('r', 'D'): 1,\n",
       " ('D', 'ö'): 5,\n",
       " ('i', 'D'): 1,\n",
       " ('r', 'S'): 5,\n",
       " ('i', 'I'): 2,\n",
       " ('ö', 'o'): 1,\n",
       " ('e', 'M'): 1,\n",
       " ('?', 'Ä'): 1,\n",
       " ('?', 'L'): 1,\n",
       " ('?', 'A'): 3,\n",
       " ('i', 'å'): 2,\n",
       " ('e', '’'): 1,\n",
       " ('’', 'n'): 1,\n",
       " ('i', 'H'): 1,\n",
       " ('t', 'S'): 1,\n",
       " ('I', 's'): 7,\n",
       " ('y', 'u'): 1,\n",
       " ('s', '-'): 1,\n",
       " ('-', 'k'): 1,\n",
       " ('u', 'S'): 69,\n",
       " ('D', 'i'): 2,\n",
       " ('n', ':'): 4,\n",
       " (':', '–'): 8,\n",
       " ('n', 'R'): 1,\n",
       " (',', 'A'): 1,\n",
       " ('u', 'I'): 1,\n",
       " ('j', 'I'): 3,\n",
       " ('j', 'å'): 1,\n",
       " ('d', 'B'): 1,\n",
       " ('y', 'b'): 1,\n",
       " ('S', 'ä'): 3,\n",
       " ('d', 'c'): 1,\n",
       " ('k', 'I'): 1,\n",
       " ('a', ':'): 2,\n",
       " ('?', 'O'): 3,\n",
       " ('r', 'I'): 11,\n",
       " ('é', 'e'): 1,\n",
       " ('_', 'S'): 1,\n",
       " ('Å', 't'): 2,\n",
       " ('j', 'j'): 3,\n",
       " ('S', 'k'): 4,\n",
       " ('V', 'o'): 1,\n",
       " ('Å', 'G'): 1,\n",
       " ('l', 'A'): 1,\n",
       " ('!', 'F'): 2,\n",
       " ('?', 'S'): 2,\n",
       " ('Å', ','): 1,\n",
       " ('!', 'O'): 4,\n",
       " ('!', 'M'): 2,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "most_freq_pair = max(pairs, key=pairs.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d', 'e')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(most_freq_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the initial symbols in a `vocabulary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = initial_vocabulary(corpus_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your most frequent pair to the vocabulary after one iteration as well as the merge operations: `merge_ops`. `merge_ops` will contain the merge operations in the order you created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ops = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "most_freq_pair = max(pairs, key=pairs.get)\n",
    "merge_ops.append(most_freq_pair)\n",
    "vocabulary.add(''.join(most_freq_pair))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 'e'),\n",
       " ('e', 'n'),\n",
       " ('a', 'n'),\n",
       " ('t', 't'),\n",
       " ('a', 'r'),\n",
       " ('s', 't'),\n",
       " ('o', 'm'),\n",
       " ('o', 'n'),\n",
       " ('l', 'l'),\n",
       " ('ö', 'r'),\n",
       " ('a', 'tt'),\n",
       " ('a', 'de'),\n",
       " ('c', 'h'),\n",
       " ('i', 'g'),\n",
       " ('n', 'g'),\n",
       " ('e', 'r'),\n",
       " ('o', 'ch'),\n",
       " ('v', 'ar'),\n",
       " ('h', 'on'),\n",
       " ('e', 't'),\n",
       " ('f', 'ör'),\n",
       " ('s', 'k'),\n",
       " ('ä', 'r'),\n",
       " ('c', 'k'),\n",
       " ('h', 'an'),\n",
       " ('o', 'r'),\n",
       " ('n', 'a'),\n",
       " ('de', 't'),\n",
       " ('s', 'å'),\n",
       " ('e', 'j'),\n",
       " ('ä', 'n'),\n",
       " ('i', 'n'),\n",
       " ('u', 'n'),\n",
       " ('de', 'n'),\n",
       " ('a', 'g'),\n",
       " ('e', 'd'),\n",
       " ('s', 'om'),\n",
       " ('i', 'll'),\n",
       " ('f', 'v'),\n",
       " ('p', 'å'),\n",
       " ('en', 'n'),\n",
       " ('i', 'd'),\n",
       " ('a', 'm'),\n",
       " ('l', 'i'),\n",
       " ('h', 'enn'),\n",
       " ('f', 'r'),\n",
       " ('henn', 'e'),\n",
       " ('h', 'ade'),\n",
       " ('a', 'll'),\n",
       " ('i', 'ng'),\n",
       " ('d', 'e')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Construction\n",
    "We will now incrementally build the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `merge_bigrams()` function that takes a list of tokens, `corpus_l`, and a pair of subword tokens `(token_r, token_l)` as input and merges adjacent sequences token_r, token_l into a new token, `token_new`, replacing the sequence `token_r, token_l` in `corpus_l`. Your function will return a new list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input \n",
    "\n",
    "`corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "\n",
    "`merge_bigrams(corpus_l, ('e', 'n'))` should return where all the seuquences of 'e' and 'n' have been merged:\n",
    "\n",
    "`['D', 'e', ' ', 's', 'en', 'a', 's', 't', ...]`\n",
    "\n",
    "And reapplying `merge_bigrams(corpus_l, ('s', 'en'))` to this corpus should return\n",
    "\n",
    "`['D', 'e', ' ', 'sen', 'a', 's', 't', ...]`\n",
    "\n",
    "You will apply a greedy algorithm. Given the pair ('a', 'a') and the list ['a', 'a', 'a'], the result will be: ['aa', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def merge_bigrams(corpus_l, pair):\n",
    "    new_corpus_l = []\n",
    "    skip = False\n",
    "    skip_next = False\n",
    "    for i in range(len(corpus_l)):\n",
    "        current = corpus_l[i]\n",
    "        next_char = corpus_l[i + 1] if i < len(corpus_l) - 1 else None\n",
    "        next_next_char = corpus_l[i + 2] if i < len(corpus_l) - 2 else None\n",
    "        if skip:\n",
    "            skip = False\n",
    "        elif skip_next:\n",
    "            skip_next = False\n",
    "        elif next_char == \" \" and (current, next_next_char) == pair:\n",
    "            new_corpus_l.append(''.join(pair))\n",
    "            skip_next = True\n",
    "            skip = True\n",
    "        elif (current, next_char) == pair:\n",
    "            new_corpus_l.append(''.join(pair))\n",
    "            skip = True\n",
    "        else:\n",
    "            new_corpus_l.append(corpus_l[i])\n",
    "    return new_corpus_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 's', 'en', 'a', 's', 't']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't']\n",
    "merge_bigrams(corpus_test, ('e', 'n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 'sen', 'a', 's', 't']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_bigrams(merge_bigrams(corpus_test, ('e', 'n')), ('s', 'en'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding (BPE): Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a `BPE()` function following Algorithm 1 in _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020). \n",
    "\n",
    "Your function will take `corpus_l` and the vocabulary size `k` as input. This size `k` will correspond to the count of new subwords added to the initial list of symbols. With your initial corpus, you should have 67 found symbols. With `k = 10`, you will add 10 subwords to this initial list. Note that Bostrom and Durrett (2020) define their $k_\\text{Bostrom and Durrett}$ as `k + initial vocabulary`. \n",
    "\n",
    "Return the vocabulary of subword tokens in the form of a list, the initial vocabulary and the subwords you will create, as well as the merge operations in the order you created them\n",
    "\n",
    "You will start from the initial vocabulary and `k` will be the number of symbols you add to this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def BPE(corpus_l, k):\n",
    "    initial_vocab = initial_vocabulary(corpus_l)\n",
    "    vocabulary = initial_vocabulary(corpus_l)\n",
    "    merge_ops = []\n",
    "    while len(vocabulary) < k + len(initial_vocab):\n",
    "        pairs = pair_count(corpus_l)\n",
    "        most_freq_pair = max(pairs, key=pairs.get)\n",
    "        while not re.fullmatch(r'[\\p{L}]+', ''.join(most_freq_pair)):\n",
    "            del pairs[most_freq_pair]\n",
    "            most_freq_pair = max(pairs, key=pairs.get)\n",
    "        merge_ops.append(most_freq_pair)\n",
    "        vocabulary.add(''.join(most_freq_pair))\n",
    "        corpus_l = merge_bigrams(corpus_l, most_freq_pair)\n",
    "    ...\n",
    "    return vocabulary, merge_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a vocabulary of 50 subwords in addition to our initial set of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "en\n",
      "an\n",
      "tt\n",
      "ar\n",
      "st\n",
      "om\n",
      "on\n",
      "ll\n",
      "ör\n",
      "att\n",
      "ade\n",
      "ch\n",
      "ig\n",
      "ng\n",
      "er\n",
      "och\n",
      "var\n",
      "hon\n",
      "et\n",
      "för\n",
      "sk\n",
      "är\n",
      "ck\n",
      "han\n",
      "or\n",
      "na\n",
      "det\n",
      "så\n",
      "ej\n",
      "än\n",
      "in\n",
      "un\n",
      "den\n",
      "ag\n",
      "ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "som\n",
      "ill\n",
      "fv\n",
      "på\n",
      "enn\n",
      "id\n",
      "am\n",
      "li\n",
      "henn\n",
      "fr\n",
      "henne\n",
      "hade\n",
      "all\n",
      "ing\n"
     ]
    }
   ],
   "source": [
    "vocabulary, merge_ops = BPE(corpus_l, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary), len(merge_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B', 'é', 'K', 'henn', 'T', 'on', ';', 'g', 'ar', 'b', 'or', 'M', '.', '9', 'll', 'och', 'ed', 'å', 'P', 'r', 'V', 'Å', 'am', '8', '’', 'd', 'O', 'y', 'enn', 'ng', 'är', 'a', '!', 'ill', 'än', 'om', 'J', 'v', 'et', 'G', 'I', 'i', '?', 'u', 'X', 'Ä', 'han', 'ck', 'D', 'S', 'en', 'hade', 'ör', 'E', 'all', 'j', 'ch', 's', 'an', 'att', '_', 'den', 'na', 'på', 'ing', 'hon', 'k', 'U', 't', 'fr', 'de', 'som', 'f', 'A', 'l', 'x', 'ä', '–', 'un', 'var', 'ö', 'Ö', 'st', 'in', 'li', ':', 'C', 'h', '1', 'ag', 'H', 'c', 'N', 'så', ',', '-', 'ade', 'fv', 'id', 'henne', 'p', 'm', 'e', 'för', 'det', 'R', 'o', 'sk', 'tt', '»', 'n', 'er', 'F', 'ig', 'z', 'L', 'ej'}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 'e'), ('e', 'n'), ('a', 'n'), ('t', 't'), ('a', 'r'), ('s', 't'), ('o', 'm'), ('o', 'n'), ('l', 'l'), ('ö', 'r'), ('a', 'tt'), ('a', 'de'), ('c', 'h'), ('i', 'g'), ('n', 'g'), ('e', 'r'), ('o', 'ch'), ('v', 'ar'), ('h', 'on'), ('e', 't'), ('f', 'ör'), ('s', 'k'), ('ä', 'r'), ('c', 'k'), ('h', 'an'), ('o', 'r'), ('n', 'a'), ('de', 't'), ('s', 'å'), ('e', 'j'), ('ä', 'n'), ('i', 'n'), ('u', 'n'), ('de', 'n'), ('a', 'g'), ('e', 'd'), ('s', 'om'), ('i', 'll'), ('f', 'v'), ('p', 'å'), ('en', 'n'), ('i', 'd'), ('a', 'm'), ('l', 'i'), ('h', 'enn'), ('f', 'r'), ('henn', 'e'), ('h', 'ade'), ('a', 'll'), ('i', 'ng')]\n"
     ]
    }
   ],
   "source": [
    "print(merge_ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use the vocabulary you obtained to tokenize a text stored in the corpus string.\n",
    "\n",
    "You will apply the merge operations in the same order you created them. You will call this function `tokenize_bpe()` that will take two inputs: `corpus` and `merge_ops`, and that will return the tokenized text in the form of a list.\n",
    "\n",
    "    def tokenize_bpe(corpus, merge_ops):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return tokens\n",
    "\n",
    "This function is easy. Just reuse the `merge_bigrams()` function and a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def tokenize_bpe(corpus, merge_ops):\n",
    "    corpus_l = list(corpus)\n",
    "    for merge_op in merge_ops:\n",
    "        corpus_l = merge_bigrams(corpus_l, merge_op)\n",
    "    return corpus_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'e', 'l', 'm', 'a', ' ', 'L', 'ag', 'er', 'l', 'ö', 'f', ' ', 'E', 'n', ' ', 'h', 'er', 'r', 'g', 'å', 'r', 'd', 's', 's', 'ä', 'g', 'en', ' ', 'B', 'o', 'k', 'u', 't', 'g', 'å', 'v', 'a', ' ', 'A', 'l', 'b', 'er', 't', ' ', 'B', 'on', 'n', 'i', 'er', 's', ' ', 'för', 'l', 'ag', ',', ' ', 'S', 't', 'o', 'ck', 'h', 'o', 'l', 'm', ' ', '1', '8', '9', '9', '.', ' ', 'I', '.', ' ', 'D', 'et', ' ', 'var', ' ', 'en', ' ', 'sk', 'ö', 'n', ' ', 'h', 'ö', 'st', 'd', 'ag', ' ', 'h', 'än', 'e', 'm', 'o', 't', ' ', 's', 'l', 'u', 't', 'et', ' ', 'a', 'f', ' ', 't', 'r', 'e', 'tt', 'i', 'o', 't', 'a', 'l', 'et', '.', ' ', 'P', 'å', ' ', 'den', ' ', 't', 'i', 'den', ' ', 'f', 'an', 'n', 's', ' ', 'i', ' ', 'U', 'p', 's', 'a', 'l', 'a', ' ', 'e', 'tt', ' ', 'h', 'ö', 'g', 't', ',', ' ', 'g', 'u', 'l', 'tt', 'v', 'å', 'v', 'å', 'n', 'ing', 's', 'h', 'u', 's', ',', ' ', 'som', ' ', 'st', 'o', 'd', ' ', 'un', 'de', 'r', 'l', 'ig', 't', ' ', 'en', 's', 'am', 't', ' ', 'på', ' ', 'en', ' ', 'li', 't', 'en', ' ', 'ä', 'ng', ',', ' ', 'l', 'å']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_bpe(corpus, merge_ops)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now done with BPE and you can now consider the unigram language model.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.2 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 2 and the related text of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the tokenization with a unigram language model as described by by Kudo (2018) and Bostrom and Durrett (2020). You will notably consider two aspects:\n",
    "1. How to obtain the subword vocabulary;\n",
    "2. How to tokenize a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, given what you have done on the byte-pair encoding, how would you build the “reasonably big seed vocabulary” needed for the unigram language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the “reasonably big seed vocabulary”, you will now fit a unigram language model. You will start with a vocabulary of 50 subwords in addition to the character set and reduce it to 49, i.e. you will find one subword to discard.\n",
    "\n",
    "Kudo (2018) proposes the expectation-maximization algorithm. We will ignore this step. Instead, in this lab, you will approximate the language model with the BPE algorithm.\n",
    "\n",
    "Write a `unigram_lm()` function that takes a corpus string and a vocabulary of subword tokens as input and returns a dictionary, where the keys are the subwords and each key value, the key relative frequency:\n",
    "\n",
    "    def unigram_lm(corpus, vocabulary):\n",
    "\n",
    "       ...\n",
    "\n",
    "      return unigram_probs\n",
    "Your function will:\n",
    "\n",
    "1. Tokenize your corpus with BPE (you can reuse the `tokenize_bpe()` function);\n",
    "2. Estimate the probability of each word (simply count the occurrences of the subwords and divide them by the length of the tokenized corpus);\n",
    "3. Return this model as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def unigram_lm(corpus, vocabulary):\n",
    "    corpus = re.sub(r'[\\s]+', ' ', corpus)\n",
    "    tokenized_corpus = tokenize_bpe(corpus, merge_ops)\n",
    "    unigram_probs = {}\n",
    "    for word in vocabulary:\n",
    "        unigram_probs[word] =  tokenized_corpus.count(word) / len(tokenized_corpus)\n",
    "    \n",
    "    ...\n",
    "    return unigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0.0006136668088577565,\n",
       " 'é': 7.483741571436055e-05,\n",
       " 'K': 0.00026193095500026193,\n",
       " 'henn': 0.00020206102242877348,\n",
       " 'T': 0.00022451224714308164,\n",
       " 'on': 0.0044004400440044,\n",
       " ';': 2.2451224714308165e-05,\n",
       " 'g': 0.017579308951303295,\n",
       " 'ar': 0.007349034223150207,\n",
       " 'b': 0.01142018963801142,\n",
       " 'or': 0.005560419987576989,\n",
       " 'M': 0.0018559679097161416,\n",
       " '.': 0.016756097378445328,\n",
       " '9': 1.496748314287211e-05,\n",
       " 'll': 0.0045351473922902496,\n",
       " 'och': 0.00811985960500812,\n",
       " 'ed': 0.004145992830575575,\n",
       " 'å': 0.01603017444601603,\n",
       " 'P': 0.00017960979771446532,\n",
       " 'r': 0.023132245197308846,\n",
       " 'V': 0.0003966383032861109,\n",
       " 'Å': 0.0003442521122860585,\n",
       " 'am': 0.0035547772464321263,\n",
       " '8': 7.483741571436055e-06,\n",
       " '’': 7.483741571436055e-06,\n",
       " 'd': 0.021074216265163932,\n",
       " 'O': 0.0012946872918584375,\n",
       " 'y': 0.005478098830291192,\n",
       " 'enn': 0.00037418707857180276,\n",
       " 'ng': 0.005567903729148425,\n",
       " 'är': 0.006129184347006129,\n",
       " 'a': 0.031102429970888246,\n",
       " '!': 0.0004714757190004715,\n",
       " 'ill': 0.00411605786428983,\n",
       " 'än': 0.004385472560861529,\n",
       " 'om': 0.007992635998293708,\n",
       " 'J': 0.0008905652470008906,\n",
       " 'v': 0.010978648885296692,\n",
       " 'et': 0.007251745582721537,\n",
       " 'G': 0.0005088944268576517,\n",
       " 'I': 0.0023573785950023575,\n",
       " 'i': 0.015251865322586681,\n",
       " '?': 0.0008905652470008906,\n",
       " 'u': 0.01665880873801666,\n",
       " 'X': 1.496748314287211e-05,\n",
       " 'Ä': 8.980489885723266e-05,\n",
       " 'han': 0.005575387470719861,\n",
       " 'ck': 0.005942090807720228,\n",
       " 'D': 0.004153476572147011,\n",
       " 'S': 0.0013021710334298736,\n",
       " 'en': 0.01788614235573217,\n",
       " 'hade': 0.003083301527431655,\n",
       " 'ör': 0.002993496628574422,\n",
       " 'E': 0.00020206102242877348,\n",
       " 'all': 0.002963561662288678,\n",
       " 'j': 0.007176908167007177,\n",
       " 'ch': 0.0011150774941439722,\n",
       " 's': 0.028423250488314136,\n",
       " 'an': 0.013350994963441923,\n",
       " 'att': 0.009833636424866976,\n",
       " '_': 0.0010177888537153035,\n",
       " 'den': 0.004160960313718447,\n",
       " 'na': 0.005096428010147954,\n",
       " 'på': 0.003913996841861057,\n",
       " 'ing': 0.002873756763431445,\n",
       " 'hon': 0.007648383886007648,\n",
       " 'k': 0.018402520524161258,\n",
       " 'U': 0.00019457728085733745,\n",
       " 't': 0.03435785755446293,\n",
       " 'fr': 0.0031656226847174515,\n",
       " 'de': 0.011861730390726147,\n",
       " 'som': 0.004123541605861266,\n",
       " 'f': 0.012684941963584113,\n",
       " 'A': 0.0004714757190004715,\n",
       " 'l': 0.024995696848596424,\n",
       " 'x': 0.00030683340442887825,\n",
       " 'ä': 0.011562380727868705,\n",
       " '–': 0.004160960313718447,\n",
       " 'un': 0.004205862763147063,\n",
       " 'var': 0.007685802593864828,\n",
       " 'ö': 0.007625932661293341,\n",
       " 'Ö': 5.2386191000052386e-05,\n",
       " 'st': 0.01231075488501231,\n",
       " 'in': 0.004370505077718656,\n",
       " 'li': 0.003397618673431969,\n",
       " ':': 8.980489885723266e-05,\n",
       " 'C': 2.2451224714308165e-05,\n",
       " 'h': 0.011854246649154711,\n",
       " '1': 7.483741571436055e-06,\n",
       " 'ag': 0.004153476572147011,\n",
       " 'H': 0.005627773661719913,\n",
       " 'c': 5.986993257148844e-05,\n",
       " 'N': 0.0004939269437147796,\n",
       " 'så': 0.004490244942861633,\n",
       " ',': 0.020520419388877664,\n",
       " '-': 7.483741571436055e-06,\n",
       " 'ade': 0.006668013740149525,\n",
       " 'fv': 0.004093606639575522,\n",
       " 'id': 0.0035622609880035625,\n",
       " 'henne': 0.0031656226847174515,\n",
       " 'p': 0.009137648458723424,\n",
       " 'm': 0.018185492018589614,\n",
       " 'e': 0.02284037927602284,\n",
       " 'för': 0.006862591021006862,\n",
       " 'det': 0.004752175897861895,\n",
       " 'R': 0.00011225612357154082,\n",
       " 'o': 0.011801860458154659,\n",
       " 'sk': 0.006391115302006391,\n",
       " 'tt': 0.007199359391721485,\n",
       " '»': 3.7418707857180274e-05,\n",
       " 'n': 0.013029194075870172,\n",
       " 'er': 0.008321920627436894,\n",
       " 'F': 0.00042657326957185515,\n",
       " 'ig': 0.008741010155437313,\n",
       " 'z': 1.496748314287211e-05,\n",
       " 'L': 0.0002843821797145701,\n",
       " 'ej': 0.004482761201290197}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs = unigram_lm(corpus, vocabulary)\n",
    "unigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply your unigram language model to tokenize a character sequence that does not include spaces, typically a single word in the Latin or Greek scripts or a sequence of words in Asian scripts, like Chinese or Korean.\n",
    "\n",
    "Write a `tokenize_lm()` function that takes a character sequence, `char_seq`, and a dictionary of unigram probabilities, `unigram_probs`,  as input and returns the subword tokens and the segmentation probability, (prob,tokens). You will only return the token list with the highest probability.\n",
    "\n",
    "    def tokenize_lm(char_seq, unigram_probs):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return max(candidates)\n",
    "\n",
    "As an example, applying \n",
    "\n",
    "`tokenize_lm('senare', unigram_probs)`\n",
    "results in\n",
    "\n",
    "`(2.0899522820189735e-07, ['s', 'en', 'ar', 'e'])`\n",
    "\n",
    "Your function will cache (memoize) the results to speed up the computation. It will be similar to that of Norvig's in the notebook: How to Do Things with Words.ipynb. You can reuse it.\n",
    "Python has a built-in memoization function that you can use: @functools.lru_cache(maxsize=2**10). You can also use the newer @functools.cache() function if you have Python 3.9 or higher. See here: https://docs.python.org/3/library/functools.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "def tokenize_lm(char_seq, unigram_probs):\n",
    "    # Use one of the two cache functions below to have a faster answer:\n",
    "    # @functools.lru_cache(maxsize=2**10)\n",
    "    @functools.cache  # Available from Python 3.9\n",
    "    def __tokenize_lm(char_seq):\n",
    "        word = char_seq\n",
    "        word_prob = unigram_probs.get(word, 0)\n",
    "        if len(char_seq) == 1:\n",
    "            return (word_prob, [word])\n",
    "        splits = [(word[:i], word[i:]) for i in range(1, len(word))]\n",
    "        splits_tok = [(__tokenize_lm(split[0]), __tokenize_lm(split[1])) for split in splits]\n",
    "        splits_tok = [(split[0][0] * split[1][0], split[0][1] + split[1][1]) for split in splits_tok]\n",
    "        splits_tok.append((word_prob, [word]))\n",
    "        return max(splits_tok, key=lambda x: x[0])\n",
    "                \n",
    "        # Write your code here\n",
    "        ...\n",
    "\n",
    "    return __tokenize_lm(char_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.533437392563631e-08, ['s', 'en', 'ar', 'e'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_lm('senare', unigram_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization with Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function applies to a sequence without spaces. You will now apply it to your corpus. Write a `tokenize_text_lm()` function that takes the whole `corpus` string as input and the unigram probabilities `unigram_probs` and return the corpus probability and the tokenized subwords. \n",
    "\n",
    "This function is just an application of the functions you just wrote, where you will:\n",
    "1. `str.split()` the string by whitespaces\n",
    "2. Break the tokens into subtokens using `tokenize_lm()`. This function will return the probabilities of the resulting sequences;\n",
    "3. Sum the logarithm of these probabilities. Use log10 to check your output with the numbers in the notebook. \n",
    "\n",
    "It is very significant that you use the logarithm of the probabilities and the sum. If you multiply the probabilities, you will get an underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize_text_lm(corpus, unigram_probs):\n",
    "    tokenized_corpus = []\n",
    "    corpus_prob = 0.0\n",
    "    for word in corpus.split():\n",
    "        word_prob, word_tok = tokenize_lm(word, unigram_probs)\n",
    "        corpus_prob += math.log10(word_prob)\n",
    "        word_tok = [\"_\" + word_tok[0]] + word_tok[1:]\n",
    "        tokenized_corpus += word_tok\n",
    "    ...\n",
    "    return corpus_prob, tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-195827.70286482148, ['_S', 'e', 'l', 'm', 'a', '_L', 'ag', 'er', 'l', 'ö'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loglikelihood, tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the final loop, where you will, at each iteration:\n",
    "1. Select one subword and remove it from your vocabulary.\n",
    "2. Estimate the probabilities of the subwords of the reduced vocabulary using `unigram_lm()`\n",
    "2. Compute the resulting log-likelihood of the corpus without this word. You will use `tokenize_text_lm()` this time\n",
    "3. Compute the loss, i.e. the log-likelihood reduction when the subword is removed from the current vocabulary\n",
    "\n",
    "You will always keep the single characters in your vocabulary to avoid unknown words.\n",
    "\n",
    "Store the pairs, (log-likelihood, removed_subword) in a list `logloss_word` and rank them by likelihood value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'a', 's', 'l', 'r', 'e', 'd', ',', 'k', 'm']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To have the same results\n",
    "vocabulary_sorted = sorted(vocabulary, key=lambda w: -unigram_probs[w])\n",
    "vocabulary_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:36<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "for word in tqdm.tqdm(vocabulary_sorted):\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.remove(word)\n",
    "    unigram_probs_temp = unigram_lm(corpus, temp_vocabulary)\n",
    "    logloss, tokens = tokenize_text_lm(corpus, unigram_probs_temp)\n",
    "    logloss_word.append((logloss, word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-195827.70286482148, 'henn'),\n",
       " (-195830.58202852483, 'enn'),\n",
       " (-196100.71907599052, 'am'),\n",
       " (-196203.833641265, 'tt'),\n",
       " (-196247.75652769106, 'ag'),\n",
       " (-196256.90154455567, 'li'),\n",
       " (-196260.5101205949, 'fr'),\n",
       " (-196295.26475448476, 'id'),\n",
       " (-196303.93999082092, 'ch'),\n",
       " (-196319.57007291287, 'ör'),\n",
       " (-196337.33252266241, 'll'),\n",
       " (-196351.2058036365, 'ed'),\n",
       " (-196360.55232036576, 'all'),\n",
       " (-196372.41403659558, 'den'),\n",
       " (-196408.1472140839, 'ing'),\n",
       " (-196418.92939948797, 'så'),\n",
       " (-196460.7473209377, 'ng'),\n",
       " (-196473.70795922022, 'hade'),\n",
       " (-196497.3970114398, 'na'),\n",
       " (-196510.03024404126, 'som'),\n",
       " (-196529.69035139968, 'det'),\n",
       " (-196548.7750217622, 'un'),\n",
       " (-196572.51740410086, 'på'),\n",
       " (-196575.2855887262, 'in'),\n",
       " (-196617.63144044214, 'fv'),\n",
       " (-196652.47750911533, 'on'),\n",
       " (-196676.91366302836, 'ej'),\n",
       " (-196701.68579294777, 'ar'),\n",
       " (-196702.65098805178, 'än'),\n",
       " (-196735.36924042017, 'sk'),\n",
       " (-196749.80994154414, 'ade'),\n",
       " (-196772.36521981275, 'ill'),\n",
       " (-196774.958114924, 'et'),\n",
       " (-196795.00903215847, 'or'),\n",
       " (-196941.6555845324, 'är'),\n",
       " (-196973.9464752186, 'han'),\n",
       " (-197104.02584617818, 'henne'),\n",
       " (-197142.3876768757, 'er'),\n",
       " (-197227.3342825548, 'ig'),\n",
       " (-197505.54635480896, 'om'),\n",
       " (-197609.0559655748, 'de'),\n",
       " (-197623.85736790954, 'st'),\n",
       " (-197817.91068932976, 'var'),\n",
       " (-197897.39099323266, 'för'),\n",
       " (-197951.00812444487, 'att'),\n",
       " (-198041.55886668697, 'hon'),\n",
       " (-198291.80458943473, 'an'),\n",
       " (-198790.79430932592, 'ck'),\n",
       " (-198855.17076376686, 'och'),\n",
       " (-200062.3987292016, 'en')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_word_1 = list(map(lambda x: ((x[0]), x[1]), logloss_word))\n",
    "sorted(logloss_word_1, reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will reduce now your vocabulary by one token: `out_candidate`. Write the piece of code to determine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "out_candidate = min(logloss_word, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-200062.3987292016, 'en')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenization without the subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jonat/progProject/Plugg/sprak-tek/edan20/labs_2023/3-BPE.ipynb Cell 104\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jonat/progProject/Plugg/sprak-tek/edan20/labs_2023/3-BPE.ipynb#Y205sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vocabulary_sorted\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39mta\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "vocabulary_sorted.remove('ta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs = unigram_lm(corpus, vocabulary_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-195827.70286482148"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-200062.3987292016"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_S', 'e', 'l', 'm', 'a', '_L', 'ag', 'er', 'l', 'ö']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Describe the background as well as the algorithms you used. For this, summarize the articles as described in the notebook:\n",
    "   * Preliminaries: subword tokenizers\n",
    "   * Design of the BPE Algorithm\n",
    "   * Unigram Language Model\n",
    "2. Describe your program as well as your results\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is September 29, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can read the sentencepiece implementation by the author:\n",
    "https://github.com/google/sentencepiece/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning BPE, you can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

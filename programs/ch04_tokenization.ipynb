{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex as re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization has no unique solution. Let us explore some possible strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us take a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tell me, O muse, of that ingenious hero who\n",
    "travelled far and wide after he had sacked the famous\n",
    "town of Troy.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first tokenizer: sequences of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r'\\p{L}+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " 'O',\n",
       " 'muse',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern1, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add the other characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = r'\\p{L}+|[^\\s\\p{L}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern2, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3 = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern3, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the punctuation as separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern4 = r'\\p{L}+|\\p{N}+|\\p{P}|[^\\s\\p{L}\\p{N}\\p{P}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern4, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using boundaries: A first tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern5 = r'\\s+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me,',\n",
       " 'O',\n",
       " 'muse,',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(pattern5, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern6 = r'([\\p{S}\\p{P}]+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(\n",
    "    pattern5,\n",
    "    re.sub(pattern6, r' \\1 ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(None, re.split(\n",
    "    pattern5,\n",
    "    re.sub(pattern6, r' \\1 ', text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nils Holgerssons underbara resa genom Sverige\\nSelma Lagerlöf\\n\\nInnehåll\\n\\tDen kristna dagvisan - Sveri'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = '../../corpus/Selma.txt'\n",
    "text = open(file_name).read().strip()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting and sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, pattern=r'\\p{L}+'):\n",
    "    words = re.findall(pattern, text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word in frequency:\n",
    "            frequency[word] += 1\n",
    "        else:\n",
    "            frequency[word] = 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze Selma Lagerlöf's novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "och \t 37799\n",
      "att \t 28914\n",
      "han \t 22743\n",
      "det \t 22087\n",
      "i \t 17072\n",
      "som \t 16790\n",
      "hade \t 14955\n",
      "på \t 14634\n",
      "hon \t 14093\n",
      "en \t 13921\n",
      "inte \t 13826\n",
      "var \t 12852\n",
      "de \t 12599\n",
      "den \t 11773\n",
      "för \t 9811\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency = count_unigrams(words)\n",
    "for word in sorted(frequency.keys(), key=frequency.get, reverse=True)[:15]:\n",
    "    print(word, '\\t', frequency[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
